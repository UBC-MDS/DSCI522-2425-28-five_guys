---
title: Predicting bike-share program usage from weather conditions
author: "Elaine Chu, Dhruv Garg, Shawn Xiao Hu, Lukman Lateef & Eugene You"
date: "2024/12/05"
jupyter: python3
format: 
    html:
        toc: true
        toc-depth: 2
    pdf:
        toc: true
        toc-depth: 2
execute:
  echo: false
  warning: false
editor: source
bibliography: references.bib
---


```{python}
import pandas as pd
from IPython.display import Markdown, display
import pickle
```

```{python}
test_scores = pd.read_csv("../results/tables/test_scores.csv")
accuracy_ridge_raw, accuracy_tree_raw = round(test_scores["accuracy_ridge"].values[0],2), round(test_scores["accuracy_tree"].values[0],2)
accuracy_ridge, accuracy_tree = f"""{round(test_scores["accuracy_ridge"].values[0],4) * 100}%""", f"""{round(test_scores["accuracy_tree"].values[0],4) * 100}%"""
```

{{< pagebreak >}}

# Summary

In this analysis, we developed two regression models - one using the decision tree algorithm and the other using the ridge regression algorithm - to compare their ability in predicting the number of bikes rented out from a bike share program depending on hourly weather conditions and general information about the day. After training the models, each model's performance was evaluated on an unseen data set, where the decision tree algorithm achieved a $R^2$ of `{python} accuracy_tree_raw` and the ridge regression algorithm obtained a $R^2$ of `{python} accuracy_ridge_raw`. These results indicate that the decision tree regression model was able to better capture the key factors influencing the number of bike rented within a reasonable range of accuracy. By utilizing weather conditions and temporal features, the model can assist bike-sharing programs in optimizing resource allocation, ensuring bikes are available when needed, and improving overall service efficiency. This makes it a practical tool for forecasting bike demand.

# Introduction

Over the past two decades, a growing number of countries worldwide have introduced bike-sharing programs as an integral part of their urban transportation systems [@shaheen2012bikesharing]. These initiatives are often designed to address the “last mile” problem – a common challenge in public transit to get passengers from a transportation hub, such as a train station or bus stop, to their final destination. By providing a sustainable, accessible, and cost-effective mode of transportation for short trips, bike-share programs have become a popular solution to close this gap [@shaheen2012bikesharing].  

The demand and usage of bike-share programs are known to be heavily influenced by the weather conditions [@eren2020review]. Factors such as temperature, precipitation, humidity, and wind speed all have an effect on the number of bikes being used at any given time. Understanding these relationships is crucial for the effective management of bike-share systems.

In this study, we explore whether a machine learning algorithm can predict the usage of a bike-share program. It is important to accurately predict the usage of bikes to ensure a stable supply of bikes to match the fluctuating demands. The efficient allocation of resources is key to the overall performance of the bike-share programs.

{{< pagebreak >}}

# Methods

## Data

The data set used in this project is the Seoul bike sharing demand data set created by Dr Sathishkumar V.E., Dr Jangwoo Park, and Dr Yongyun Cho from Sunchon National University [@sathishkumar2020using]. It was sourced from the UCI Machine Learning Repository [@dua2017uci] and can be found [here](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand). Each row in the data set represents the number of bikes being rented at a specific hour of day, along with the corresponding weather conditions (e.g. temperature, humidity, and rainfall), whether the day was a holiday, and if the bike share program was operating that day.

## Analysis

We developed and compared two regression models that predict the number of bikes being rented out for a specific hour of the day. One was built using the decision tree regressor algorithm, and the other using the ridge regression algorithm. All variables from the original data set, except for `Dew point temperature`, were used to fit the model. The data was split into a training and test set at a 70:30 ratio. The `Date` column from the original data set was split up into temporal features of `Year`, `Month`, `Day`, and `Weekday`. These features along with `Hour` and `Seasons` will be treated as categorical features, and will be encoded by One-hot encoding before fitting to the model. In addition, features that indicated events (`Holiday`, `Functioning Day`) were transformed and treated to be handled as binary categorical features. All the other numeric features related to weather will be scaled using standardization just before model fitting. For the decision regressor algorithm the hyperparameters of tree depth, minimum samples per split, and minimum sample per leaf were optimized based on the $R^2$ score through a 5-fold cross-validation. For the ridge regression algorithm, the hyperparameter `alpha` was also optimized based on the $R^2$ score through a 5-fold random search cross-validation.

The Python programming language [@Python] and the following Python packages were used to perform the analysis: numpy [@harris2020array], Pandas [@mckinney2010data], altair [@altair], vegafusion [@kruchten2022vegafusion], scikit-learn [@pedregosa2011scikit], requests [@requests], and deepchecks [@chorev2022deepchecks]. The code used to perform the analysis and create this report can be found [here](https://github.com/UBC-MDS/DSCI522-2425-28-rental-bike-prediction).

# Results

To identify which features would be useful in predicting the number of bikes used, we began with some exploratory data analysis (EDA) to better understand the dataset's general structure and characteristics. We found that there were no missing values in the dataset, eliminating the need for any imputation or data cleaning related to missing data. The descriptions from the EDA show the dataset includes both numerical and categorical variables (e.g., `Seasons`, `Holiday`), some of which may interact non-linearly with the target variable (@tbl-summary-stats). 

When plotting bike usage against features such as the hour of the day, seasonality, temperature, and holiday status, we noticed non-uniform patterns in their distribution with some categories with little overlap on each other (@fig-season-hourly, @fig-season-temp-count, @fig-holiday_dist). We observed spikes in the number of bikes rented in mornings, evenings, and during summer months. (@fig-season-hourly). This suggests that these features might serve as strong predictors on bike usage. 

We also plotted a correlation matrix between all the features (@fig-corr_chart). We found a strong correlation between `Temperature` and `Dew point temperature`. Given that `Dew point temperature` values closely mirror those of temperature, we decided to drop `Dew point temperature` from the model. The remaining features showed relatively displayed relatively weaker correlations, so we chose to preserve them as they may offer valuable information for predicting bike demand.




## Exploratory Data Analysis

After the train and test set split, we carried out a very comprehensive EDA and found that there were no missing values in the dataset, eliminating the need for any preprocessing, imputation or data cleaning related to missing data. Hence, we delved deeper into understanding the distribution of the training data set which we have the summary statistics in (@tbl-summary-stats) below followed by the distribution and correlation charts.
<!-- ```{python}
#| label: tbl-missing-values
#| tbl-cap: Number of missing values in every column
missing_values = pd.read_csv("../results/tables/missing_values.csv")
Markdown(missing_values.to_markdown(index = False))
``` -->


```{python}
#| label: tbl-summary-stats
#| tbl-cap: Summary statistics of key columns 
summary_stats = pd.read_csv("../results/tables/summary_stats.csv")
summary_stats1, summary_stats2 = summary_stats.iloc[:,:7].round(2), summary_stats.iloc[:,7:].round(2)
Markdown(summary_stats1.to_markdown(index = False))
```


<!-- ![Distribution of Rented Bike Count](../results/figures/rented_bike_count.png){#fig-rented-bike-count width=70%} -->

@fig-season-hourly below shows the distribution of bike rentals across different hours of the day and seasons. It can be seen that bike rentals follow a bimodal distribution, peaking during typical commuting hours (8 AM and 5–6 PM), signifying rush hour when people are going to work or school and there is a greater peak in the evening when most people return home after their daily engagements. Summer experiences the highest rentals throughout the day, especially in the evening. This pattern highlights strong seasonal variation and the influence of daily commuting behavior on bike rentals.

![Average number of bikes rented at different seasons and time of the day.](../results/figures/season_hourly.png){#fig-season-hourly width=70%}

<!-- ![Average Rented Bike Count by Hour](../results/figures/hourly_rental_count.png){#fig-hourly-rental-count width=70%} -->


<!-- ![Average Rented Bike Count by Season](../results/figures/season_rental_count.png){#fig-season-rental-count width=70%} -->

The number of bike rentals increases with temperature, peaking in the Summer season as seen in @fig-season-temp-count below. Conversely, the Winter season shows low rentals, especially at sub-zero temperatures. Spring and Autumn show moderate rental activity, corresponding to mild temperatures. This plot highlights clear seasonal variation in bike rentals influenced by temperature.

![Number of bike rentals at different temperatures for different seasons](../results/figures/season_temp_count.png){#fig-season-temp-count width=70%}

It can be observed from @fig-holiday_dist that during holidays, bike rentals are lower overall, and the spread is narrower with median rental count much lower (~200) when compared to non-holidays median rental count (~500). Non-holidays see significantly higher bike rentals compared to holidays, with a wider distribution and higher median. This difference can be attributed to typical commuting behavior, where non-holidays (weekdays) experience more regular demand for bike rentals compared to holidays.

![Summary distribution of bike rentals between holidays and non holidays.](../results/figures/holiday_dist.png){#fig-holiday_dist width=70%}

<!-- ![Distribution of number of rented bikes.](../results/figures/rented_bike_count.png){#fig-rented-bike-count width=70%} -->

@fig-corr_chart compares Pearson correlations (left) and Spearman correlations (right) among multiple variables, including `Rented Bike Count`, weather conditions, and time-related features. The Spearman correlation captures additional non-linear relationships between the features, while Pearson emphasizes linear trends. For example, temperature shows a strong positive correlation with `Rented Bike Count` in both Pearson and Spearman analyses while negative correlation in humidity, suggests higher humidity reduces bike rentals. These insights highlight the importance of weather conditions and time in predicting bike rental demand.

![Correlation plots for the different features in the training data set.](../results/figures/corr_chart.png){#fig-corr_chart}

{{< pagebreak >}}

## Modeling

We implemented two regression models with a decision tree regressor algorithm and a ridge regression algorithm. To optimize the models performance in predicting bike usage, we performed a randomized 5-fold cross validation using the $R^2$ as the performance metric to evaluate and select the best value for hyperparameters. For the decision tree regressor algorithm, we specifically tuned the maximum depth of the tree, the minimum number of samples required to split a node, and the minimum samples required per leaf. For the ridge regression algorithm, the `alpha` value was tuned during the cross validation. Based on the analysis, we found out that the optimal decision tree regressor model configuration had a maximum depth of 20, a minimum of 10 sample to split a node, and a minimum 4 samples per leaf. Whereas for the ridge regression algorithm, the optimal `alpha` value was 10.

![Predicted bike usage by Decision Tree model vs actual bike usage from test data set.](../results/figures/prediction_error_tree.png){#fig-error-tree width=70%}

![Predicted bike usage by Regression model vs actual bike usage from test data set.](../results/figures/prediction_error_ridge.png){#fig-error-ridge width=70%}

The decision tree regression model did moderately well on the test data, with an $R^2$ of `{python} accuracy_tree_raw`. Compared to the ridge regression model, it only obtained a $R^2$ of `{python} accuracy_ridge_raw`. This can also be seen by the scatter plot of the predicted bike usage versus the actual bike usage data in @fig-error-tree. The ridge regression model has a wide spread of prediction for the entire range of bike usage as seen on @fig-error-ridge, yet the decision tree regression model seems to be predicting the lower numbers well, albeit still struggling in predicting higher bike usage with the same precision. Despite this, since the bulk of the data lies within a lower range and this is the range where the model performs well, the model should still be able to provide a reasonably accurate predictions for most of the observations.

# Discussion

The results reflect the performance of two different models trained on the dataset: a Ridge Regression model and a Decision Tree Regressor. The Decision Tree model outperforms the Ridge Regression model in explaining the variance in the target variable (`{python} accuracy_tree` vs. `{python} accuracy_ridge`), this suggests that the data might have non-linear relationships that the Decision Tree is better able to capture compared to the linear Ridge Regression model.

While the Decision Tree achieves better performance, it might be more prone to overfitting if the hyperparameters (e.g., `max_depth`, `min_samples_split`, and `min_samples_leaf`) are not carefully tuned. The constraints imposed by the chosen hyperparameters reduce this risk. Ridge Regression, being a linear model, is less prone to overfitting but struggles to model complex, non-linear relationships.

The dataset includes both numerical and categorical variables (e.g., `Seasons`, `Holiday`), some of which may interact non-linearly with the target variable. This could explain the superior performance of the Decision Tree model. Features like `Temperature`, `Humidity`, and `Visibility` likely exhibit non-linear effects on the bike rental count, which Ridge Regression cannot model effectively.

The Decision Tree Regressor demonstrates better performance than Ridge Regression in terms of $R^2$ score, indicating that it better explained the variance in bike rentals, highlighting the importance of non-linear models for this dataset. However, proper interpretability, regularization, and validation are necessary to ensure the model’s robustness. The dataset’s complexity suggests that exploring more advanced ensemble methods could yield even better results.

# References
