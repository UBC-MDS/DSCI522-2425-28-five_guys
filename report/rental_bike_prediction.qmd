---
title: Predicting bike-share program usage from weather conditions
author: "Elaine Chu, Dhruv Garg, Shawn Xiao Hu, Lukman Lateef & Eugene You"
date: "2024/12/05"
jupyter: python3
format: 
    html:
        toc: true
        toc-depth: 2
    pdf:
        toc: true
        toc-depth: 2
execute:
  echo: false
  warning: false
editor: source
bibliography: references.bib
---


```{python}
import pandas as pd
from IPython.display import Markdown, display
import pickle
```

```{python}
test_scores = pd.read_csv("../results/tables/test_scores.csv")
accuracy_ridge_raw, accuracy_tree_raw = round(test_scores["accuracy_ridge"].values[0],2), round(test_scores["accuracy_tree"].values[0],2)
accuracy_ridge, accuracy_tree = f"""{round(test_scores["accuracy_ridge"].values[0],4) * 100}%""", f"""{round(test_scores["accuracy_tree"].values[0],4) * 100}%"""
```

{{< pagebreak >}}

# Summary

In this analysis, we developed two regression models using the decision tree algorithm and the ridge regression algorithm to compare their ability to predict the number of bikes being rented out from a bike share program based on the weather conditions of the hour and general information about the day. After training the models, the models' performances were evaluated on an unseen data set, where the decision tree algorithm achieved a $R^2$ of `{python} accuracy_tree_raw` and the ridge regression algorithm obtained a $R^2$ of `{python} accuracy_ridge_raw`. The results indicated that the decision tree regression model was able to better capture the key factors influencing the number of bike rented within a reasonable range of accuracy. This makes it a practical tool for forecasting bike demand. By utilizing weather conditions and temporal features, this model can assist bike-sharing programs in optimizing resource allocation, ensuring bikes are available when needed, and improving overall service efficiency. 

# Introduction

Over the past two decades, a growing number of countries worldwide have introduced bike-sharing programs as an integral part of their urban transportation systems [@shaheen2012bikesharing]. These initiatives are often designed to address the “last mile” problem – a common challenge in public transit to get passengers from a transportation hub, like train stations and bus stops, to their final destination. By providing a sustainable, accessible, and cost-effective mode of transportation for short trips, bike-share programs have become a popular solution to close this gap [@shaheen2012bikesharing].  

The demand and usage of bike-share programs are known to be heavily influenced by the weather conditions [@eren2020review]. Factors such as temperature, precipitation, humidity, and wind speed all have an effect on the number of bikes being used at any given time. Understanding these relationships is crucial for the effective management of bike-share systems.

In this study, we explore whether a machine learning algorithm can predict the usage of bike-share program. It is important to accurately predict usage of the bikes as it gives organizers the ability to plan ahead and make sure there is a stable supply of bikes to match the fluctuating demands. This ensures an efficient allocation of resources and ultimately improve the overall performance of the bike-share programs.

{{< pagebreak >}}

# Methods

## Data

The data set used in this project is the Seoul bike sharing demand data set created by Dr Sathishkumar V.E., Dr Jangwoo Park, and Dr Yongyun Cho from Sunchon National University [@sathishkumar2020using]. It was sourced from the UCI Machine Learning Repository [@dua2017uci] and can be found [here](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand). Each row in the data set represents the number of bikes being rented at a specific hour of a day, along with corresponding weather conditions (e.g. temperature, humidity, and rainfall), whether the day was a holiday, and if the bike share program was functioning that day.

## Analysis

We developed and compared two regression models that predict the number of bikes being rented out for a specific hour of the day one is built using the decision tree regressor algorithm another using the ridge regression algorithm. All variables, except for `dew point temp`, from the original data set were used to fit the model. The data was split into training and test set at a 70:30 ratio. The `Date` column from the original data set was split up into temporal features of `Year`, `Month`, `Day`, and `Weekday`. These features along with  `Hour` and `Seasons` will be treated as categorical features and will be encoded by One-hot encoding before fitting to the model. In addtion, features that indicated events (`Holiday`, `Functioning Day`) were transformed and treated so they can be treated as binary categorical features. All the other numeric features that were related to the weather will be scaled using standardization just before model fitting. For the decision regressor algorithm the hyperparameters `tree depth`, `minimum samples per split`, and `minimum sample per leaf` were optimized based on the $R^2$ score through a 5-fold cross-validation. For the ridge regression algorithm, the hyperparameter `alpha` was also optimized based on the $R^2$ score through a 5-fold random search cross-validation.

The Python programming language [@Python] and the following Python packages were used to perform the analysis: numpy [@harris2020array], Pandas [@mckinney2010data], altair [@altair], vegafusion [@kruchten2022vegafusion], scikit-learn [@pedregosa2011scikit], requests [@requests] and deepchecks [@chorev2022deepchecks]. The code used to perform the analysis and create this report can be found here: [Rental Bike Prediction Repo](https://github.com/UBC-MDS/DSCI522-2425-28-rental-bike-prediction) 

# Results

To identify which features may be useful to predict the number of bikes being used, we began with some exploratory data analysis (EDA) to better understand the dataset's general structure and characteristics. We found that there were no missing values in the dataset, eliminating the need for any imputation or data cleaning related to missing data. The descriptions from the EDA shows the dataset includes both numerical and categorical variables (e.g., “Seasons,” “Holiday”), some of which may interact non-linearly with the target variable. 

When plotting bike usage against features such as the hour of the day, seasonality, temperature, and holiday status, we noticed non-uniform patterns in their distribution with some categories having little overlap on each other. We observed that there is a spike in the number of bikes rented in summer seasons and also there is a spike in the morning and evening of each day observed. This suggests that these features might serve as strong predictors on bike usage. 

We also plotted a correlation matrix between all the features, we found a strong correlation between temperature and dew point temperature. Given that dew point temperature values closely mirror those of temperature, we decided to drop dew point temperature from the model. The remaining features showed relatively weaker correlations, so we chose not to remove them as they may offer valuable information for predicting bike demand.




## Exploratory Data Analysis

After the train and test set split, we carried out a very comprehensive EDA and found that there were no missing values in the dataset, eliminating the need for any preprocessing, imputation or data cleaning related to missing data. Hence, we delved deeper into understanding the distribution of the training data set which we have the summary statistics in @tbl-summary-stats.
<!-- ```{python}
#| label: tbl-missing-values
#| tbl-cap: Number of missing values in every column
missing_values = pd.read_csv("../results/tables/missing_values.csv")
Markdown(missing_values.to_markdown(index = False))
``` -->


```{python}
#| label: tbl-summary-stats
#| tbl-cap: Summary statistics of key columns 
summary_stats = pd.read_csv("../results/tables/summary_stats.csv")
summary_stats1, summary_stats2 = summary_stats.iloc[:,:7].round(2), summary_stats.iloc[:,7:].round(2)
Markdown(summary_stats1.to_markdown(index = False))
```


![Distribution of Rented Bike Count](../results/figures/rented_bike_count.png){#fig-rented-bike-count width=70%}


<!-- ![Average Rented Bike Count by Hour](../results/figures/hourly_rental_count.png){#fig-hourly-rental-count width=70%} -->


<!-- ![Average Rented Bike Count by Season](../results/figures/season_rental_count.png){#fig-season-rental-count width=70%} -->

![Number of bike rentals at different temperatures for different seasons](../results/figures/season_temp_count.png){#fig-season-temp-count width=70%}

![Summary distribution of bike rentals between holidays and non holidays.](../results/figures/holiday_dist.png){#fig-holiday_dist width=70%}

<!-- ![Distribution of number of rented bikes.](../results/figures/rented_bike_count.png){#fig-rented-bike-count width=70%} -->

![Mean number of bikes being rented at different time of the day.](../results/figures/season_hourly.png){#fig-season-hourly width=70%}

![Correlation plots for the different features in the training data set.](../results/figures/corr_chart.png){#fig-corr_chart}

{{< pagebreak >}}

## Modeling

We implemented two regression models with a decision tree regressor algorithm and a ridge regression algorithm. To optimize the models performance in predicting bike usage, we performed a randomized 5-fold cross validation using the $R^2$ as the performance metric to evaluate and select the best value for hyperparameters. For the decision tree regressor algorithm, we specifically tuned the maximum depth of the tree, the minimum number of samples required to split a node, and the minimum samples required per leaf. For the ridge regression algorithm, the alpha value was tuned during the cross validation. Based on the analysis, we found out that the optimal decision tree regressor model configuration had a maximum depth of 20, a minimum of 10 sample to split a node, and a minimum 4 samples per leaf. Whereas for the ridge regression algorithm, the optimal alpha value was 10.

![Predicted bike usage by Decision Tree model vs actual bike usage from test data set.](../results/figures/prediction_error_tree.png){#fig-error-tree width=70%}

![Predicted bike usage by Regression model vs actual bike usage from test data set.](../results/figures/prediction_error_ridge.png){#fig-error-ridge width=70%}

The decision tree regression model did moderately well on the test data, with an $R^2$ of `{python} accuracy_tree_raw`. Compared to the ridge regression model, it only obtained a $R^2$ of `{python} accuracy_ridge_raw`. This can also be seen by the scatter plot of the predicted bike usage versus the actual bike usage data in @fig-error-tree. The ridge regression model has a wide spread of prediction for the entire range of bike usage as seen on @fig-error-ridge, yet the decision tree regression model seems to be predicting the lower numbers well, albeit still struggling in predicting higher bike usage with the same precision. Despite this, since the bulk of the data lies within a lower range and this is the range where the model performs well, the model should still be able to provide a reasonably accurate predictions for most of the observations.

# Discussion

The results reflect the performance of two different models trained on the dataset: a Ridge Regression model and a Decision Tree Regressor. The Decision Tree model outperforms the Ridge Regression model in explaining the variance in the target variable (`{python} accuracy_tree` vs. `{python} accuracy_ridge`), this suggests that the data might have non-linear relationships that the Decision Tree is better able to capture compared to the linear Ridge Regression model.

While the Decision Tree achieves better performance, it might be more prone to overfitting if the hyperparameters (e.g., max_depth, min_samples_split, and min_samples_leaf) are not carefully tuned. The constraints imposed by the chosen hyperparameters reduce this risk. Ridge Regression, being a linear model, is less prone to overfitting but struggles to model complex, non-linear relationships.

The dataset includes both numerical and categorical variables (e.g., “Seasons,” “Holiday”), some of which may interact non-linearly with the target variable. This could explain the superior performance of the Decision Tree model. Features like “Temperature,” “Humidity,” and “Visibility” likely exhibit non-linear effects on the bike rental count, which Ridge Regression cannot model effectively.

The Decision Tree Regressor demonstrates better performance than Ridge Regression in terms of $R^2$ score, indicating that it better explained the variance in bike rentals, highlighting the importance of non-linear models for this dataset. However, proper interpretability, regularization, and validation are necessary to ensure the model’s robustness. The dataset’s complexity suggests that exploring more advanced ensemble methods could yield even better results.

# References
